# üöÄ Implementaci√≥n de Pipeline ETL Serverless & CI/CD

**Rama:** `feature/configuracion-inicial-ale`
**Responsable:** Alejandro Nelson Herrera Soria
**Tickets Asociados:** `KAN-34` (AWS Glue), `KAN-33` (CI/CD)

-----

## üìë Tabla de Contenidos

1.  [Resumen Ejecutivo](https://www.google.com/search?q=%23-resumen-ejecutivo)
2.  [Arquitectura y Decisiones de Dise√±o](https://www.google.com/search?q=%23-arquitectura-y-decisiones-de-dise%C3%B1o)
3.  [Detalle de Implementaci√≥n: ETL con AWS Glue](https://www.google.com/search?q=%23-detalle-de-implementaci%C3%B3n-etl-con-aws-glue)
4.  [Detalle de Implementaci√≥n: CI/CD Pipeline](https://www.google.com/search?q=%23-detalle-de-implementaci%C3%B3n-cicd-pipeline)
5.  [Desaf√≠os T√©cnicos y Soluciones (Troubleshooting)](https://www.google.com/search?q=%23-desaf%C3%ADos-t%C3%A9cnicos-y-soluciones)
6.  [Estructura del C√≥digo](https://www.google.com/search?q=%23-estructura-del-c%C3%B3digo)

-----

## üìã Resumen Ejecutivo

Esta rama consolida la infraestructura de procesamiento de datos y control de calidad del proyecto. Se ha implementado un **Pipeline ETL Serverless** utilizando **AWS Glue** para la transformaci√≥n de datos crudos (Bronze) a datos limpios y optimizados (Silver), y se ha establecido un flujo de **Integraci√≥n Continua (CI)** mediante **GitHub Actions** para garantizar la estandarizaci√≥n del c√≥digo Python.

-----

## üèó Arquitectura y Decisiones de Dise√±o

### 1\. Procesamiento Serverless con AWS Glue (KAN-34)

Optamos por **AWS Glue** (sobre soluciones basadas en EC2 como Airflow workers) bas√°ndonos en tres pilares:

  * **Escalabilidad Horizontal:** Glue gestiona autom√°ticamente los recursos (Workers/Executors) de Spark. Si el volumen de datos crece de Gigabytes a Terabytes, el Job escala sin intervenci√≥n manual.
  * **Optimizaci√≥n de Costos:** Modelo de pago por uso (Serverless). Solo incurrimos en costos durante los minutos de ejecuci√≥n del ETL, evitando el gasto de servidores EC2 ociosos las 24 horas.
  * **Mantenibilidad:** Se elimina la carga operativa de parchear sistemas operativos, gestionar memoria RAM o configurar cl√∫steres de Spark manualmente.

### 2\. Almacenamiento Optimizado (Silver Layer)

  * **Formato:** **Parquet**. Elegido por su naturaleza columnar, ideal para consultas anal√≠ticas (OLAP), reduciendo dr√°sticamente el tiempo de I/O comparado con CSV o JSON.
  * **Compresi√≥n:** **Snappy**. Ofrece el mejor balance entre ratio de compresi√≥n y velocidad de descompresi√≥n para ecosistemas Hadoop/Spark.
  * **Particionamiento:** Datos organizados por `year` o `province` para habilitar el *Partition Pruning* en consultas futuras (Athena/PowerBI), minimizando costos de escaneo.

### 3\. Calidad de C√≥digo Automatizada (KAN-33)

Implementamos un **Quality Gate** en el repositorio para prevenir deuda t√©cnica:

  * **Linter:** `flake8` para detecci√≥n temprana de errores de sintaxis y bugs potenciales.
  * **Formatter:** `black` para asegurar un estilo de c√≥digo consistente y legible (PEP 8).
  * **Import Sorter:** `isort` para organizar dependencias.

-----

## üõ† Detalle de Implementaci√≥n: ETL con AWS Glue

Los scripts de ETL (`src/glue/`) realizan la transici√≥n **Bronze ‚Üí Silver** aplicando las siguientes reglas de negocio y limpieza t√©cnica:

1.  **Esquema Dictatorial (Schema Enforcement):**

      * Se definen esquemas manuales (`StructType`) para cada fuente. Esto blinda al pipeline contra el *Schema Drift* (cambios inesperados en los tipos de datos de la fuente).
      * Se ignoran columnas t√©cnicas irrelevantes o corruptas del origen.

2.  **Normalizaci√≥n de Tipos:**

      * Casteo expl√≠cito de campos num√©ricos (`Double`, `Long`) y fechas.
      * Manejo de inconsistencias en la ingesta (ej. fechas guardadas como `INT64`).

3.  **Estandarizaci√≥n de Nombres:**

      * Conversi√≥n de columnas a `snake_case` (ej. `Country Name` ‚Üí `country_name`) para facilitar el uso en SQL.

**Jobs Desarrollados:**

  * `job_bronze_to_silver_world_bank.py`: Procesa indicadores socioecon√≥micos.
  * `job_bronze_to_silver_jmp.py`: Procesa datos de Agua y Saneamiento (WHO/UNICEF).
  * `job_bronze_to_silver_weather.py`: Procesa datos clim√°ticos hist√≥ricos (Open-Meteo).

-----

## üîÑ Detalle de Implementaci√≥n: CI/CD Pipeline

Se configur√≥ un Workflow de GitHub Actions (`.github/workflows/ci.yml`) que se dispara autom√°ticamente en cada `Push` o `Pull Request` hacia la rama `main`.

**Pasos del Pipeline:**

1.  Levanta un contenedor Ubuntu con Python 3.10.
2.  Instala dependencias de calidad: `black`, `flake8`, `isort`.
3.  Ejecuta formateo y linting sobre el c√≥digo fuente en `src/`.
4.  **Bloqueo:** Si se detectan errores cr√≠ticos, el Pipeline falla, alertando al equipo antes de fusionar el c√≥digo defectuoso.

-----

## üí• Desaf√≠os T√©cnicos y Soluciones

Durante el desarrollo nos enfrentamos a inconsistencias cr√≠ticas en la capa Bronze (Ingesta). A continuaci√≥n se documentan las soluciones aplicadas:

| Desaf√≠o / Error | Causa Ra√≠z | Soluci√≥n Implementada |
| :--- | :--- | :--- |
| **Schema Drift / Merge Failure**<br>`[CANNOT_MERGE_SCHEMAS]` | Los archivos Parquet en Bronze ten√≠an tipos de datos mixtos (ej. columna `scale` a veces era `INT`, a veces `STRING` o `NULL`) debido a la inferencia din√°mica de Pandas en la ingesta. | Implementaci√≥n de **Lectura con Esquema Manual** (`spark.read.schema(...)`). Esto fuerza a Spark a ignorar la inferencia y adherirse estrictamente al tipo de dato esperado. |
| **Conflicto de Particiones**<br>`COLUMN_ALREADY_EXISTS` | Existencia de columnas en el archivo (ej. `country`) con el mismo nombre que las carpetas de partici√≥n (`country=ARG`), generando ambig√ºedad en el Cat√°logo de Datos. | Uso de **`recursiveFileLookup`** y lectura directa desde S3 (bypasseando el Cat√°logo de Glue) para ignorar la estructura de carpetas y leer solo el contenido de los archivos. |
| **Formatos H√≠bridos**<br>`Not a Parquet file` | Se detect√≥ que algunos datasets en la carpeta Bronze eran archivos CSV planos, a pesar de estar en una estructura de Data Lake. | Adaptaci√≥n din√°mica del lector de Spark (`.csv` con headers) dentro del Job de JMP, manteniendo la salida estandarizada en Parquet para la capa Silver. |
| **Inferencia de Fechas**<br>`INT64 vs Timestamp` | Las fechas fueron guardadas como enteros (microsegundos) sin metadatos de tiempo. | Lectura inicial como `LongType` y transformaci√≥n matem√°tica (`col/1000000`) a `Timestamp` dentro del ETL. |

-----

## üìÇ Estructura del C√≥digo

```text
huella-hidrica/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml          # Definici√≥n del Pipeline de Calidad (GitHub Actions)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ glue/               # Scripts PySpark (ETL Jobs)
‚îÇ       ‚îú‚îÄ‚îÄ job_bronze_to_silver_world_bank.py
‚îÇ       ‚îú‚îÄ‚îÄ job_bronze_to_silver_jmp.py
‚îÇ       ‚îî‚îÄ‚îÄ job_bronze_to_silver_weather.py
‚îú‚îÄ‚îÄ .gitignore              # Exclusiones (venv, terraform state, etc.)
‚îî‚îÄ‚îÄ README.md               # Esta documentaci√≥n
```

-----

*Documentaci√≥n generada para el Sprint 1 del Proyecto Final - Data Engineering.*
