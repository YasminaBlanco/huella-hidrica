# üíß"Huella H√≠drica de Am√©rica Latina"

### Dise√±o e Implementaci√≥n de un Pipeline para Vulnerabilidad H√≠drica y Social

---

## üí° 1. Contexto y Objetivos

Millones de personas en Am√©rica Latina enfrentan un acceso limitado al agua potable y saneamiento adecuado, agravado por el cambio clim√°tico y la desigualdad socioecon√≥mica. Este proyecto, impulsado por una ONG, busca transformar la toma de decisiones en la regi√≥n.

### Objetivo General
Construir una **Plataforma de Datos abierta y escalable (Data Lake en AWS)** que integre informaci√≥n cr√≠tica para generar insights accionables, orientando la inversi√≥n y las pol√≠ticas p√∫blicas hacia los territorios con mayor **vulnerabilidad h√≠drica, sanitaria y social**.

### üéØ Preguntas Clave que Responde la Plataforma
* Impacto del clima en el acceso al agua.
* Identificaci√≥n de zonas cr√≠ticas para inversi√≥n en infraestructura.
* Riesgo sanitario y social asociado a la falta de saneamiento y pobreza.
* Relaci√≥n entre desarrollo econ√≥mico y acceso al agua.
* Brechas urbano‚Äìrural en servicios WASH.

## üìê 2. Arquitectura de Datos y Stack Tecnol√≥gico

El sistema implementa un *pipeline* **ELT (Extract, Load, Transform)** sobre una **Arquitectura Medallion** en **Amazon S3**, utilizando herramientas de c√≥digo abierto para el procesamiento y la orquestaci√≥n.

### Stack Tecnol√≥gico

| Categor√≠a | Tecnolog√≠a Principal | Prop√≥sito |
| :--- | :--- | :--- |
| **Plataforma Cloud** | **AWS (S3, EC2, IAM)** | Almacenamiento escalable del Data Lake
| **Data Lake** | **Amazon S3** | Arquitectura Medallion (Bronze, Silver, Gold). Formato **Parquet** optimizado. |
| **Transformaci√≥n** | **Apache Spark (PySpark)** | Limpieza, estandarizaci√≥n y modelado dimensional (desplegado en Docker/EC2). |
| **Orquestaci√≥n** | **Apache Airflow** | Programaci√≥n, monitoreo y encadenamiento de *jobs* ETL (desplegado en Docker/EC2). |
| **Ingesta** | **FastAPI Microservices** | Microservicios dedicados para APIs y fuentes de datos espec√≠ficas. |

---

## üó∫Ô∏è 3. Fuentes de Datos y Alcance

| Fuente | Contenido | Granularidad | Alcance Geogr√°fico |
| :--- | :--- | :--- | :--- |
| **JMP (WHO/UNICEF)** | Acceso a Agua y Saneamiento (WASH). | Pa√≠s, A√±o (Desagregaci√≥n Urbano/Rural). | Pa√≠ses de Am√©rica Latina. |
| **Open-Meteo** | Variables clim√°ticas hist√≥ricas (Precipitaci√≥n, Temperatura). | Diario (Agregado a Mensual/Anual). | M√©xico, Argentina, Brasil, Chile. |
| **World Bank** | Indicadores Socioecon√≥micos (PIB p/C√°pita, Pobreza, Mortalidad infantil). | Pa√≠s, A√±o. | Pa√≠ses de Am√©rica Latina. |

---

## ‚öôÔ∏è 4. Gu√≠a de Instalaci√≥n y Ejecuci√≥n (Core Infrastructure)

La arquitectura se despliega utilizando **Docker Compose** en instancias **AWS EC2** (m√≠nimo 8GB RAM, 2 vCPUs).

### 4.1. Configuraci√≥n del Servidor

Se recomienda usar **tres instancias EC2 (Spark, Kafka, Airflow)**.

* **Instancias EC2 (Ubuntu 24.04 LTS):** Configurar y descargar la clave `.pem`. Asignar el perfil **IAM** requerido para acceder a S3.
* **Conexi√≥n SSH y Actualizaci√≥n:**
    ```bash
    ssh -i tu-clave.pem usuario@ip-publica
    sudo apt update && sudo apt upgrade -y
    ```
* **Instalar Docker y Docker Compose:**
    ```bash
    sudo apt install docker.io docker-compose -y
    sudo usermod -aG docker $USER 
    # ¬°CERRAR Y VOLVER A ABRIR SESI√ìN SSH!
    ```

### 4.2. Configuraci√≥n del Proyecto

* **Clonar el Repositorio:**
    ```bash
    git clone [https://github.com/tu-usuario/proyecto-integrador.git](https://github.com/tu-usuario/proyecto-integrador.git)
    cd proyecto-integrador
    ```
* **Configurar Variables de Entorno (`.env`):**
    Editar el archivo `.env` con las credenciales y nombres de *Buckets* S3:
    ```bash
    # AWS
    AWS_ACCESS_KEY_ID=TU_ACCESS_KEY
    AWS_SECRET_ACCESS_KEY=TU_SECRET_KEY
    # Nombres de Buckets S3
    BUCKET_NAME_RAW=TU_BUCKET_RAW
    BUCKET_NAME_SILVER=TU_BUCKET_SILVER
    BUCKET_NAME_GOLD=TU_BUCKET_GOLD
    S3_REGION=TU_REGION
    ```

### 4.3. Despliegue y Ejecuci√≥n

Para entornos distribuidos, la carpeta correspondiente debe copiarse a su respectiva instancia EC2 (*spark/* a la instancia Spark, etc.).

* **Construir y Levantar los Contenedores:**
    ```bash
    docker-compose up -d --build
    ```
* **Verificar el Estado:**
    ```bash
    docker-compose ps
    ```

### 4.4. Acceso a Airflow UI

* **URL:** `http://<IP_P√öBLICA>:8080`
* Acceda a la interfaz, cargue las conexiones y **desbloquee** el DAG principal para iniciar el *pipeline* Bronze ‚Üí Silver ‚Üí Gold.

---

## üìÑ 5. Documentaci√≥n por M√≥dulo

Para una comprensi√≥n profunda de la l√≥gica de cada componente, por favor, consulte los `README.md` individuales:

* **[Infraestructura (Terraform) ¬ª](terraform/README.md)**: Estructura de la infraestructura.
* **[Ingesta (FastAPI) ¬ª](ingest/readme.md)**: Detalle sobre la extracci√≥n de datos de las APIs de World Bank y Open-Meteo y la carga a la capa Bronze.
* **[(Spark) ¬ª](spark/readme.md)**: Como levantar el contenedor de Spark, ejecutar los scripts de ETL y visualizar los resultados.
* **[Silver¬ª](spark/app/silver/readme.md)**: Estructura de la capa Silver y los scripts de limpieza.
* **[Gold¬ª](spark/app/gold/readme.md)**: Estructura de la capa Gold y los scripts de modelado.
* **[Orquestaci√≥n (Airflow) ¬ª](orch/readme.md)**: Estructura de los DAGs, dependencias y manejo de errores.
