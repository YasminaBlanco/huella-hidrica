##  GuÃ­a de configuraciÃ³n (EC2 + Docker) y ejecuciÃ³n en Spark 
Se explica paso a paso cÃ³mo desplegar un entorno de *Apache Spark 4.0* dentro de un contenedor *Docker* corriendo en una *EC2, con acceso seguro a **S3 via IAM Role, y cÃ³mo ejecutar un **job de prueba* que lee datos desde Bronze y escribe a Silver.

AsÃ­ cÃ³mo ejecutar los distintos **jobs de transformaciÃ³n** del proyecto de huella hÃ­drica:

- Limpieza Bronze â†’ Silver  
- Modelado Silver (dimensiones / hechos)  
- Capa Gold (KPIs) 

## Arquitectura del proyecto 

```text
spark/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ gold/                              # Jobs de capa Gold (KPIs)
â”‚   â”‚   â”œâ”€â”€ gold_kpi01_climate_water.py
â”‚   â”‚   â”œâ”€â”€ gold_kpi02_water_mobility.py
â”‚   â”‚   â”œâ”€â”€ gold_kpi03_critical_zones.py
â”‚   â”‚   â”œâ”€â”€ gold_kpi04_health_risk_population.py
â”‚   â”‚   â”œâ”€â”€ gold_kpi05_urban_rural_gap_water.py
â”‚   â”‚   â”œâ”€â”€ gold_kpi06_water_gdp_corr.py
â”‚   â”‚   â””â”€â”€ gold_kpi07_water_sanitation_gap.py
â”‚   â”‚   â””â”€â”€ readme.md                 # DocumentaciÃ³n tecnica de la capa Gold
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ dims_facts/                    # Modelado Silver (dimensiones y hechos)
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_dims_job.py
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_fact_climate_annual.py
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_fact_climate_monthly.py
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_fact_socioeconomic.py
â”‚   â”‚   â”‚   â””â”€â”€ silver_fact_wash_coverage_jmp.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ limpieza_transf/              # Limpieza Bronze â†’ Silver
â”‚   â”‚   â”‚   â”œâ”€â”€ etl_strategies.py         # Job de limpieza para datos de World Bank y Open Mateo 
â”‚   â”‚   â”‚   â”œâ”€â”€ jmp_silver_job.py         # Job de limpieza para datos JMP
â”‚   â”‚   â”‚   
â”‚   â”‚   â””â”€â”€ readme.md                 # Modelo relacional y metadatos de la capa Silver
â”‚   â”‚
â”‚   â”œâ”€â”€ test_spark.py                     # Job de prueba
â”‚   â”œâ”€â”€ base_gold_model_job.py            # Clase base para jobs Gold (KPIs)
â”‚   â”œâ”€â”€ base_silver_job.py                # Clase base para jobs de limpieza Silver
â”‚   â”œâ”€â”€ base_silver_model_job.py          # Clase base para jobs de modelado Silver
â”‚   â”œâ”€â”€ main_silver.py                    # Orquestador: Bronze â†’ Silver (clean)
â”‚   â”œâ”€â”€ main_silver_model.py              # Orquestador: Silver model (dims/facts)
â”‚   â”œâ”€â”€ main_gold_model.py                # Orquestador: KPIs Gold
â”‚   
â”‚
â”œâ”€â”€ conf/
â”‚   â””â”€â”€ spark-defaults.conf               # ConfiguraciÃ³n de Spark (S3, performance)
â”‚
â”œâ”€â”€ Dockerfile                            # Imagen base de Spark y dependencias
â””â”€â”€ docker-compose.yml                    # OrquestaciÃ³n del contenedor de Spark
â””â”€â”€ readme.md                             # GuÃ­a de ejecuciÃ³n del pipeline en Spark
```
## Requisitos:
- Cuenta AWS con permisos para:
- EC2: Ubuntu 22.04 (tamaÃ±o t3.large o superior recomendado).
- Docker + Contenedor Spark (bitnami/spark base).
- IAM Role asociado a la EC2 con permisos sobre los buckets

## Preparar la instancia (Ubuntu + Docker)

- Instalar Docker

``` bash
## Accede por SSH
ssh -i <tu-key.pem> ubuntu@<EC2_PUBLIC_IP>

## Instala Docker 
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg lsb-release
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \
  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
  https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io

## (Opcional) Usa Docker sin sudo (requiere reiniciar sesiÃ³n)
sudo usermod -aG docker $USER

## Instala el plugin Docker Compose V2 (docker compose)
sudo apt-get install -y docker-compose-plugin

## Verifica las versiones instaladas
docker --version
docker compose version

```
### Construir y levantar Spark

```bash
# Ir a la carpeta del proyecto
cd ~/spark-elt

# Construir la imagen de Spark
sudo docker compose build

# Levantar el contenedor
sudo docker compose up -d

# Verificar que el contenedor estÃ© corriendo
sudo docker ps
```

### Ejecutar el job de prueba (Bronze â†’ Silver)
```bash
# Entrar al contenedor de Spark
sudo docker exec -it spark bash

# Ir al workspace dentro del contenedor
cd /opt/elt/app

# Ejecutar el script de prueba
spark-submit test_spark.py
```
## Pipeline de transformaciÃ³n completo

A continuaciÃ³n se muestran los comandos que ejecutan los jobs reales de transformaciÃ³n, en el orden correcto:

- Limpieza y estandarizaciÃ³n: Bronze â†’ Silver
- Modelado Silver (dimensiones y hechos)
- Modelo Gold (KPIs)

ğŸ” En estos ejemplos se usa:

- `BASE_BUCKET=henry-pf-g2-huella-hidrica`
- `PROCESS_YEAR=2025`
- `PROCESS_MONTH=12`

Puedes cambiar `PROCESS_YEAR` y `PROCESS_MONTH` segÃºn el mes/aÃ±o que quieras procesar.

- Bronze â†’ Silver (limpieza)

```bash
docker exec -it spark bash -lc '\
  cd /opt/elt/app && \
  export BASE_BUCKET=henry-pf-g2-huella-hidrica && \
  export PROCESS_YEAR=2025 && \
  export PROCESS_MONTH=12 && \
  /opt/bitnami/spark/bin/spark-submit --master local[*] main_silver.py \
'
```
Este comando:

- Usa el bucket configurado en `BASE_BUCKET`.
- Procesa el periodo indicado por `PROCESS_YEAR` y `PROCESS_MONTH`.
- Ejecuta la lÃ³gica de limpieza y estandarizaciÃ³n, leyendo desde `bronze/` y escribiendo en `silver/`.

- Silver Model (dimensiones y hechos)

```bash
docker exec -it spark bash -lc '\
  cd /opt/elt/app && \
  export BASE_BUCKET=henry-pf-g2-huella-hidrica && \
  export PROCESS_YEAR=2025 && \
  export PROCESS_MONTH=12 && \
  /opt/bitnami/spark/bin/spark-submit --master local[*] main_silver_model.py \
'
```
Este job:

- Parte de la capa Silver limpia.
- Construye las tablas modelo de Silver (dimensiones, hechos y vistas intermedias) siguiendo el diseÃ±o dimensional del proyecto.

- Silver Model â†’ Gold (KPIs)

```bash
docker exec -it spark bash -lc '\
  cd /opt/elt/app && \
  export BASE_BUCKET=henry-pf-g2-huella-hidrica && \
  /opt/bitnami/spark/bin/spark-submit --master local[*] main_gold_model.py \
'
```
Este comando:

- Lee exclusivamente desde las tablas modelo de Silver.
- Genera todas las tablas de la capa Gold, con los KPIs que consumen los dashboards (Streamlit / BI).



